---
version: '3'

tasks:
  bootstrap:
    desc: Bootstrap a new cluster
    cmds:
      - |
        kubectl apply --server-side --kustomize {{.CLUSTER_DIR}}/bootstrap/crds/
        kubectl apply --server-side --kustomize {{.CLUSTER_DIR}}/bootstrap/flux
        kubectl apply -f {{.CLUSTER_DIR}}/flux/vars/cluster-settings.yaml
        kubectl create secret generic  -n flux-system cluster-secrets --from-env-file <(doppler secrets download -p cluster -c prd --no-file --format docker)
        kubectl create secret generic -n security \
            doppler-token-auth-api \
            --from-literal dopplerToken=$(doppler secrets -p doppler -c prd get DOPPLER_PAT --plain)
        kubectl apply --server-side --kustomize {{.CLUSTER_DIR}}/flux/config

  schemas:
    desc: Pull the latest CRD schemas for this cluster
    cmds:
      - |
        mkdir -p {{.CLUSTER_DIR}}/schemas
        flux pull artifact oci://ghcr.io/onedr0p/kubernetes-schemas-oci:latest \
            --output={{.CLUSTER_DIR}}/schemas

  mount:
    desc: Mount a PersistantVolumeClaim to a pod temporarily
    interactive: true
    vars:
      claim: '{{ or .claim (fail "PersistentVolumeClaim `claim` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    cmds:
      - |
        kubectl run -n {{.namespace}} debug-{{.claim}} -i --tty --rm --image=null --privileged --overrides='
          {
            "apiVersion": "v1",
            "spec": {
              "containers": [
                {
                  "name": "debug",
                  "image": "ghcr.io/onedr0p/alpine:rolling",
                  "command": [
                    "/bin/bash"
                  ],
                  "stdin": true, e
                  "stdinOnce": true,
                  "tty": true,
                  "volumeMounts": [
                    {
                      "name": "config",
                      "mountPath": "/data/config"
                    }
                  ]
                }
              ],
              "volumes": [
                {
                  "name": "config",
                  "persistentVolumeClaim": {
                    "claimName": "{{.claim}}"
                  }
                }
              ],
              "restartPolicy": "Never"
            }
          }'
    preconditions:
      - kubectl -n {{.namespace}} get pvc {{.claim}}
  delete-failed-pods:
    desc: Deletes failed pods
    cmds:
      - kubectl delete pods --field-selector status.phase=Failed -A --ignore-not-found=true

  alpine:
    desc: Run alpine container
    cmds:
      - kubectl run -it alpine --image alpine:3.15.4

  dnsutils:
    desc: Run DNSUtils pod
    cmds:
      - kubectl run -it dnsutils --image gcr.io/kubernetes-e2e-test-images/dnsutils:1.3

  what-dockerhub:
    desc: What dockerhub images are running in my cluster
    cmds:
      - kubectl get pods --all-namespaces -o=jsonpath="{range .items[*]}{'\n'}{range .spec.containers[*]}{.image}{'\n'}{end}{end}" | sort | uniq | grep -Ev 'quay|gcr|ghcr|ecr|us-docker' | grep -Ev 'bitnami|rook|intel|grafana' |  sed -e 's/docker\.io\///g' | sort | uniq

  debug-networking:
    desc: Create a netshoot container for debugging
    cmds:
      - kubectl run tmp-shell --rm -i --tty --image docker.io/nicolaka/netshoot:latest {{.CLI_ARGS}}

  debug-volume:
    desc: Attach a volume to a container for debugging, ex. VOLUME=zigbee2mqtt-config-v1 NAMESPACE=home task debug-volume
    interactive: true
    cmds:
      - defer: kubectl -n ${NAMESPACE} delete pod debug-${VOLUME}
      - |
        cat <<EOF | kubectl apply -f -
        kind: Pod
        apiVersion: v1
        metadata:
          name: "debug-${VOLUME}"
          namespace: "${NAMESPACE}"
          labels:
            volume: "${VOLUME}"
        spec:
          containers:
            - name: debug
              image: docker.io/library/alpine:3.15
              command: ["/bin/sh"]
              tty: true
              lifecycle:
                postStart:
                  exec:
                    command:
                      - /bin/sh
                      - -c
                      - apk add --no-cache curl nano
              volumeMounts:
                - name: data
                  mountPath: /data
                - name: backups
                  mountPath: /mnt/backups
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: "${VOLUME}"
            - name: backups
              nfs:
                server: "hades"
                path: "/volume1/K8S/main"
        EOF
      - kubectl -n $NAMESPACE wait --for=condition=ready pod -l volume=$VOLUME
      - kubectl -n $NAMESPACE exec (kubectl get pod -n $NAMESPACE -l volume=$VOLUME -o name) -it debug -- /bin/sh
