---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: plex
  namespace: media
spec:
  releaseName: plex
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://k8s-at-home.com/charts/
      chart: plex
      version: 6.2.0
      sourceRef:
        kind: HelmRepository
        name: k8s-at-home-charts
        namespace: flux-system
      interval: 5m
  values:
    # Values link: https://github.com/k8s-at-home/charts/blob/master/charts/stable/plex/values.yaml
    image:
      repository: ghcr.io/k8s-at-home/plex
      tag: v1.25.2.5319-c43dc0277
    env:
      TZ: "${TZ}"
      ADVERTISE_IP: "https://plex.${CLUSTER_DOMAIN}:443/,http://${LB_PLEX}:32400/"
      PLEX_CLAIM: "${PLEX_TOKEN}"

    podSecurityContext:
      supplementalGroups:
        - 44
        - 109
        - 100
        - 65539
    service:
      main:
        enabled: true
        type: LoadBalancer
        loadBalancerIP: ${LB_PLEX}
        externalTrafficPolicy: Local
        annotations:
          metallb.universe.tf/allow-shared-ip: plex

    # hostNetwork: true

    ingress:
      main:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: traefik
          cert-manager.io/cluster-issuer: ${CLUSTER_CERT}
          external-dns/is-public: "true"
          external-dns.alpha.kubernetes.io/target: ipv4.${CLUSTER_DOMAIN}
          external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          hajimari.io/enable: "true"
          hajimari.io/icon: plex
          hajimari.io/appName: plex
        hosts:
          - host: "plex.${CLUSTER_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
        tls:
          - secretName: plex-${CLUSTER_NAME}-tls
            hosts:
              - "plex.${CLUSTER_DOMAIN}"

    persistence:
      config:
        enabled: true
        existingClaim: plex-config-v1

      transcode:
        enabled: true
        type: emptyDir
        medium: Memory

      nfs-nas-media:
        enabled: true
        type: custom
        volumeSpec:
          nfs:
            server: ${NAS_ADDRESS}
            path: /volume1/
        mountPath: /data
        readOnly: false

      backup:
        enabled: true
        type: custom
        volumeSpec:
          nfs:
            server: ${NAS_ADDRESS}
            path: /volume1/backup/kubernetes/apps/plex
        mountPath: /config/backup
        readOnly: false

    podAnnotations:
      backup.velero.io/backup-volumes: config
      pre.hook.backup.velero.io/container: fsfreeze
      pre.hook.backup.velero.io/command: '["/sbin/fsfreeze", "--freeze", "/config"]'
      post.hook.backup.velero.io/container: fsfreeze
      post.hook.backup.velero.io/command: '["/sbin/fsfreeze", "--unfreeze", "/config"]'
      configmap.reloader.stakater.com/reload: "plex-promtail"

    resources:
      requests:
        cpu: 1000m
        memory: 1000Mi
        gpu.intel.com/i915: 1
      limits:
        memory: 16000Mi
        gpu.intel.com/i915: 1

    additionalContainers:
      fsfreeze:
        name: fsfreeze
        image: ghcr.io/k8s-at-home/fsfreeze:v2.37-r0
        volumeMounts:
          - name: config
            mountPath: /config
        securityContext:
          privileged: true

    # addons:
    #   promtail:
    #     enabled: true
    #     image:
    #       repository: grafana/promtail
    #       tag: 2.4.1
    #     loki: http://loki.logs.svc.cluster.local:3100/loki/api/v1/push
    #     logs:
    #     - name: promtail/plex
    #       path: "/config/Library/Application Support/Plex Media Server/Logs/*.log"
    #     - name: promtail/plex/plugins
    #       path: "/config/Library/Application Support/Plex Media Server/Logs/PMS Plugin Logs/*.log"
    #     volumeMounts:
    #     - name: config
    #       mountPath: /config
    #       readOnly: true
    #     securityContext:
    #       runAsUser: 0
